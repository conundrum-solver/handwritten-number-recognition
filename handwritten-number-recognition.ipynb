{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b313c76-20b0-415e-8ef7-0d474edbcf88",
   "metadata": {},
   "source": [
    "# Detecting handwritten numbers using TensorFlow\n",
    "In this project, I will create a deep learning model using TensorFlow to detect handwritten numbers. The model will be trained, validated, and tested using the MNIST dataset, which is a large collection of handwritten digits widely used for training various image processing systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c696a3b-0fc8-458d-8f52-9258ad7e9e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2998f3f2-d976-407c-8a55-e38bcb836095",
   "metadata": {},
   "source": [
    "## Step 1: Data Preparation\n",
    "The MNIST dataset consists of 60,000 training images and 10,000 testing images of handwritten digits from 0 to 9. Each image is a 28x28 grayscale image. The dataset is already labeled, which means each image comes with the correct digit label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c243240c-9f69-4b2d-aabc-ba82e43ea277",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(\n",
    "    name='mnist',\n",
    "    split=['train', 'test'],\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "mnist_train, mnist_test = mnist_dataset[0], mnist_dataset[1]\n",
    "\n",
    "# Setting the training and validation sample sizes\n",
    "num_validation_samples = int(0.1 * mnist_info.splits[\"train\"].num_examples)\n",
    "num_test_samples = int(mnist_info.splits[\"test\"].num_examples)\n",
    "\n",
    "# Scaling inputs to be between 0 and 1\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "scaled_test_data = mnist_test.map(scale)\n",
    "\n",
    "# Shuffling the data\n",
    "BUFFER_SIZE = 10000\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# Assigning the validation and training datasets\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "# Setting the batch size for the training data and updating the training, validation and test datasets\n",
    "BATCH_SIZE = 200\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = scaled_test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12ff8c0-cf17-4406-b757-2417e9f91166",
   "metadata": {},
   "source": [
    "## Step 2: Building the Model\n",
    "I will use TensorFlow, an open-source deep learning framework, to build the neural network model. The architecture of the model will include:\n",
    "\n",
    "Input Layer: This layer will accept the 28x28 pixel values of the images.\n",
    "Hidden Layers: Several dense (fully connected) layers with ReLU & tahn activation functions will be used. These layers will help the model learn complex patterns in the data.\n",
    "Output Layer: The final layer will have 10 neurons with a softmax activation function, representing the probability distribution of the 10 digit classes (0-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6cb58895-8107-4312-9904-1529bbe48191",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 200\n",
    "\n",
    "# The model has 3 hidden layers and an output layer. I'll use a combination of the Rectified Linear Unit (ReLU) & hyperbolic tangent (tahn) activation functions for the hidden layers, and the softmax activatin function for the output to produce a probability.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation=\"tanh\"),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation=\"tanh\"),\n",
    "    tf.keras.layers.Dense(output_size, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0f27d6-4e69-4728-aec1-8fc8649a1886",
   "metadata": {},
   "source": [
    "## Step 3: Training and Validating the Model\n",
    "The model will be trained on the MNIST training dataset. The process involves:\n",
    "\n",
    "Loss Function: Categorical Crossentropy will be used as the loss function to measure the difference between the predicted and actual labels.\n",
    "Optimiser: The Adam optimiser will be utilized to adjust the learning rate dynamically and improve the model's accuracy.\n",
    "Metrics: Accuracy will be the primary metric for evaluating the model's performance.\n",
    "\n",
    "To ensure the model is not overfitting and generalises well to unseen data, I will validate the model using a portion of the training dataset as a validation set. This will help in tuning hyperparameters and improving the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "09f658b4-1390-44d0-ba48-9e834fc3f755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To optimise the model I'll use Adaptive Moment Estimation (ADAM) optimiser as it combines the benefits of RMSprop and Stochastic Gradient Descent with momentum.\n",
    "# The loss function I'll be using is Sparse Catetorical Crossentropy which applies one-hot encoding so that the output shape matches the target shape.\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cef55bf1-f2e5-4c49-b765-2ead673e3ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "270/270 - 3s - 11ms/step - accuracy: 0.9145 - loss: 0.2871 - val_accuracy: 0.9605 - val_loss: 0.1373\n",
      "Epoch 2/5\n",
      "270/270 - 2s - 9ms/step - accuracy: 0.9681 - loss: 0.1042 - val_accuracy: 0.9755 - val_loss: 0.0894\n",
      "Epoch 3/5\n",
      "270/270 - 2s - 7ms/step - accuracy: 0.9786 - loss: 0.0689 - val_accuracy: 0.9800 - val_loss: 0.0682\n",
      "Epoch 4/5\n",
      "270/270 - 2s - 8ms/step - accuracy: 0.9837 - loss: 0.0511 - val_accuracy: 0.9823 - val_loss: 0.0610\n",
      "Epoch 5/5\n",
      "270/270 - 2s - 8ms/step - accuracy: 0.9881 - loss: 0.0369 - val_accuracy: 0.9872 - val_loss: 0.0454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x156a8c57210>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS_NUM = 5\n",
    "#To train the model, I'll set the parameters as below. The training cycles (epochs) are set to 5 epochs. I'll also add the validation data and targets to validate the model.\n",
    "model.fit(train_data, epochs = EPOCHS_NUM, validation_data = (validation_inputs, validation_targets), verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f91c36-add2-4771-961b-50f07dccc48d",
   "metadata": {},
   "source": [
    "## Step 4: Testing the Model\n",
    "Finally, the model will be tested on the MNIST test dataset to evaluate its accuracy and robustness. The performance metrics obtained from the test data will give an indication of how well the model can recognize handwritten digits in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3431980c-e2cf-4cf4-a58d-29a9e484f756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.9766 - loss: 0.0793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07927960157394409, 0.9765999913215637]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0f6ef8-3f34-42fe-96fc-fdc83b8ca97a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The model accuracy based onthe test dataset is 97.66% compared to 98.81% and 98.72% for the training and validation datasets respectively. This shows that the model's performance on the test dataset is very close to the training and validation datasets. Moreover, the test loss is 0.0793 compared to 0.0369 and 0.0454 for the training and validation datasets respectively. This shows a very close result indicating that I haven't overfitted the data during the training and validation process.\n",
    "This will conclude the development of this model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
